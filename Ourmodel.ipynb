{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import urandom\n",
    "from  keras. callbacks  import  ModelCheckpoint, LearningRateScheduler\n",
    "from  keras. models  import  Model\n",
    "from  keras. layers  import  Dense, Conv1D, Conv2D,Input, ReLU,Reshape, Permute, Add, Flatten, BatchNormalization, Activation, Dropout,DepthwiseConv2D\n",
    "from keras.regularizers import l2\n",
    "import  matplotlib. pyplot  as  plt\n",
    "import time\n",
    "from  keras  import  layers\n",
    "import tensorflow as tf\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sbox = np.array([0xc, 0x5, 0x6, 0xb, 0x9, 0x0, 0xa, 0xd, 0x3, 0xe, 0xf, 0x8, 0x4, 0x7, 0x1, 0x2])\n",
    "\n",
    "raw_P = [0,  16, 32, 48, 1,  17, 33, 49, 2,  18, 34, 50, 3,  19, 35, 51,\n",
    "     4,  20, 36, 52, 5,  21, 37, 53, 6,  22, 38, 54, 7,  23, 39, 55,\n",
    "     8,  24, 40, 56, 9,  25, 41, 57, 10, 26, 42, 58, 11, 27, 43, 59,\n",
    "     12, 28, 44, 60, 13, 29, 45, 61, 14, 30, 46, 62, 15, 31, 47, 63]\n",
    "raw_P = np.array(raw_P)\n",
    "\n",
    "# Big-Edian\n",
    "index = np.array([63 - i for i in range(64)])\n",
    "raw_P = 63 - raw_P[index]\n",
    "\n",
    "P = np.array([np.where(raw_P == i) for i in range(64)])\n",
    "P = np.squeeze(P)\n",
    "\n",
    "# for decryption, to be test\n",
    "Sbox_inverse = np.array([0x5, 0xe, 0xf, 0x8, 0xc, 0x1, 0x2, 0xd, 0xb, 0x4, 0x6, 0x3, 0x0, 0x7, 0x9, 0xa])\n",
    "P_inverse = raw_P\n",
    "\n",
    "# for updating keys\n",
    "KP = np.array([(i+61) % 80 for i in range(80)])\n",
    "\n",
    "\n",
    "# x shape: (-1, 4)\n",
    "def get_Sbox_output_enc(x):\n",
    "    n, m = np.shape(x)\n",
    "    assert m == 4\n",
    "    x_val = x[:, 0] * 8 + x[:, 1] * 4 + x[:, 2] * 2 + x[:, 3]\n",
    "    y_val = Sbox[x_val]\n",
    "    output = np.zeros((n, 4), dtype=np.uint8)\n",
    "    for i in range(4):\n",
    "        output[:, i] = (y_val >> (3 - i)) & 1\n",
    "    # print('y_val shape is ', np.shape(y_val))\n",
    "    return output\n",
    "\n",
    "\n",
    "# x shape: (-1, 4)\n",
    "def get_Sbox_output_dec(x):\n",
    "    n, m = np.shape(x)\n",
    "    assert m == 4\n",
    "    x_val = x[:, 0] * 8 + x[:, 1] * 4 + x[:, 2] * 2 + x[:, 3]\n",
    "    y_val = Sbox_inverse[x_val]\n",
    "    output = np.zeros((n, 4), dtype=np.uint8)\n",
    "    for i in range(4):\n",
    "        output[:, i] = (y_val >> (3 - i)) & 1\n",
    "    # print('y_val shape is ', np.shape(y_val))\n",
    "    return output\n",
    "\n",
    "\n",
    "# keys shape: (-1, 80)\n",
    "def update_master_key(keys, round_counter):\n",
    "    tp = keys[:, KP]\n",
    "    new_keys = copy.deepcopy(tp)\n",
    "    new_keys[:, :4] = get_Sbox_output_enc(tp[:, :4])\n",
    "    round_counter_arr = np.array([(round_counter >> (4-i)) & 1 for i in range(5) ], dtype=np.uint8)\n",
    "    new_keys[:, 60:65] = tp[:, 60:65] ^ round_counter_arr\n",
    "    return new_keys\n",
    "\n",
    "\n",
    "# keys shape: (-1, 80)\n",
    "def expand_key(keys, nr):\n",
    "    n, m = np.shape(keys)\n",
    "    assert m == 80\n",
    "    ks = np.zeros((nr+1, n, 64), dtype=np.uint8)\n",
    "    ks[0] = keys[:, :64]\n",
    "    for i in range(1, nr+1):\n",
    "        keys = update_master_key(keys, i)\n",
    "        ks[i] = keys[:, :64]\n",
    "    return ks\n",
    "\n",
    "\n",
    "# x shape: (-1, 64)\n",
    "def sBoxLayer_enc(x):\n",
    "    n, m = np.shape(x)\n",
    "    assert m == 64\n",
    "    output = np.zeros((n, 64), dtype=np.uint8)\n",
    "    for i in range(16):\n",
    "        st = 4 * i\n",
    "        output[:, st:st+4] = get_Sbox_output_enc(x[:, st:st+4])\n",
    "    return output\n",
    "\n",
    "\n",
    "# x shape: (-1, 64)\n",
    "def sBoxLayer_dec(x):\n",
    "    n, m = np.shape(x)\n",
    "    assert m == 64\n",
    "    output = np.zeros((n, 64), dtype=np.uint8)\n",
    "    for i in range(16):\n",
    "        st = 4 * i\n",
    "        output[:, st:st+4] = get_Sbox_output_dec(x[:, st:st+4])\n",
    "    return output\n",
    "\n",
    "\n",
    "# x shape: (-1, 64)\n",
    "def pLayer_enc(x):\n",
    "    output = x[:, P]\n",
    "    return output\n",
    "\n",
    "\n",
    "# x shape: (-1, 64)\n",
    "def pLayer_dec(x):\n",
    "    output = x[:, P_inverse]\n",
    "    return output\n",
    "\n",
    "\n",
    "# x shape: (-1, 64)\n",
    "# subkeys shape: (-1, 64)\n",
    "def enc_one_round(x, subkeys):\n",
    "    y = sBoxLayer_enc(x)\n",
    "    z = pLayer_enc(y)\n",
    "    output = z ^ subkeys\n",
    "    return output\n",
    "def dec_one_round(x, subkeys):\n",
    "    y = pLayer_dec(x)\n",
    "    z = sBoxLayer_dec(y)\n",
    "    output = z ^ subkeys\n",
    "    return output\n",
    "\n",
    "\n",
    "def encrypt(x, ks):\n",
    "    nr = ks.shape[0]\n",
    "    y = x ^ ks[0]\n",
    "    for i in range(1, nr):\n",
    "        y = enc_one_round(y, ks[i])\n",
    "    return y\n",
    "def decrypt(x, ks):\n",
    "    nr = ks.shape[0]\n",
    "    y = x ^ ks[nr-1]\n",
    "    for i in range(1, nr):\n",
    "        y = dec_one_round(y, ks[nr - 1 - i])\n",
    "    return y\n",
    "def decrypt_1(x):\n",
    "    y = pLayer_dec(x)\n",
    "    z = sBoxLayer_dec(y)\n",
    "    return y,z\n",
    "def decrypt_2(x):\n",
    "    y = pLayer_dec(x)\n",
    "#     z = sBoxLayer_dec(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def make_train_data(subkeyy,n=10**7, nr=9):\n",
    "    x0 = np.frombuffer(urandom(n * 8), dtype=np.uint64)  # .reshape(-1, 1)\n",
    "    p0 = np.zeros((n, 64), dtype=np.uint8)\n",
    "    for i in range(64):\n",
    "        off = 63 - i\n",
    "        p0[:, i] = (x0 >> off) & 1\n",
    "    arr = [[0, 0, 0, 1], [0, 0, 1, 0], [0, 0, 1, 1], [0, 1, 0, 0], [0, 1, 0, 1], [0, 1, 1, 0],\n",
    "           [0, 1, 1, 1], [1, 0, 0, 0], [1, 0, 0, 1], [1, 0, 1, 0], [1, 0, 1, 1], [1, 1, 0, 0], [1, 1, 0, 1],\n",
    "           [1, 1, 1, 0], [1, 1, 1, 1]]\n",
    "#     master_keys = np.frombuffer(urandom(n * 80), dtype=np.uint8).reshape(-1, 80) & 1\n",
    "    subkeys = expand_key(subkeyy, nr)\n",
    "    c0 = encrypt(p0, subkeys)\n",
    "\n",
    " \n",
    "\n",
    "    #生成正样本\n",
    "    pp_1=p0.copy()\n",
    "    for j in range(4):\n",
    "        pp_1[:,j]=pp_1[:,j]^arr[0][j]\n",
    "    c0_1 = encrypt(pp_1, subkeys)\n",
    "    c0_1_xor=np.bitwise_xor(c0,c0_1)\n",
    "\n",
    "#     c0_0=decrypt_1(c0)\n",
    "    c0_1_xor_2,c0_1_xor=decrypt_1(c0_1)\n",
    "#     creal=c0_1_xor.copy()\n",
    "#     creal_1=c0_1_xor_2.copy()\n",
    "    creal=np.concatenate((c0,c0_1),axis=1) #解密一轮在异或\n",
    "#     creal_1=np.concatenate((c0_1,c0_1_xor),axis=1)\n",
    "    # creal_0=np.concatenate((c0,c0_1),axis=1) #解密一轮直接做\n",
    "    for i in range(1,14,2):\n",
    "        pp=p0.copy()\n",
    "        pp_1=p0.copy()\n",
    "        for j in range(4):\n",
    "            pp[:,j]=pp[:,j]^arr[i][j]\n",
    "            pp_1[:,j]=pp_1[:,j]^arr[i+1][j]\n",
    "        c1=encrypt(pp, subkeys)\n",
    "        c1_1=encrypt(pp_1, subkeys)\n",
    "\n",
    "        c1_xor=np.bitwise_xor(c0,c1)\n",
    "        c1_1_xor=np.bitwise_xor(c0,c1_1)\n",
    "        c1_xor_tran_2,c1_xor_tran=decrypt_1(c1)\n",
    "        c1_1_xor_tran_2,c1_1_xor_tran=decrypt_1(c1_1)\n",
    "        \n",
    "        creal=np.concatenate((creal,c1,c1_1),axis=1)\n",
    "#         creal_1=np.concatenate((creal,c1_xor_tran_2,c1_1_xor_tran_2),axis=1)\n",
    "\n",
    "#         creal=np.concatenate((creal,np.bitwise_xor(c0_0,c1_xor),np.bitwise_xor(c0_0,c1_1_xor)),axis=1)#解密一轮在异或\n",
    "#         creal_1=np.concatenate((creal_1,np.bitwise_xor(c0_1,c1),np.bitwise_xor(c0_1,c1_1)),axis=1)\n",
    "        # creal_0=np.concatenate((creal_0,c1,c1_1),axis=1)#解密一轮直接做\n",
    "    #生成负样本\n",
    "    x1 = np.frombuffer(urandom(n * 8), dtype=np.uint64)\n",
    "    p1 = np.zeros((n, 64), dtype=np.uint8)\n",
    "    x1_1 = np.frombuffer(urandom(n * 8), dtype=np.uint64)\n",
    "    p1_1 = np.zeros((n, 64), dtype=np.uint8)\n",
    "    for i in range(64):\n",
    "        off = 63 - i\n",
    "        p1[:, i] = (x1 >> off) & 1\n",
    "        p1_1[:, i] = (x1_1 >> off) & 1\n",
    "    c2=encrypt(p1, subkeys)\n",
    "    c2_1=encrypt(p1_1, subkeys)\n",
    "\n",
    "    c2_1_xor=np.bitwise_xor(c2,c2_1)\n",
    "\n",
    "#     c2_2=decrypt_1(c2)\n",
    "    c2_1_xor_2,c2_1_xor=decrypt_1(c2_1)\n",
    "#     crand=c2_1_xor.copy()\n",
    "#     crand_2=c2_1_xor_2.copy()\n",
    "    crand=np.concatenate((c2,c2_1),axis=1)  #异或值\n",
    "#     crand_1=np.concatenate((c2_1,np.bitwise_xor(c2,c2_1)),axis=1)  #异或值\n",
    "    # crand_0=np.concatenate((c2,c2_1),axis=1)\n",
    "    for j in range(7):\n",
    "        x1 = np.frombuffer(urandom(n * 8), dtype=np.uint64)\n",
    "        p1 = np.zeros((n, 64), dtype=np.uint8)\n",
    "        x1_1 = np.frombuffer(urandom(n * 8), dtype=np.uint64)\n",
    "        p1_1 = np.zeros((n, 64), dtype=np.uint8)\n",
    "        for i in range(64):\n",
    "            off = 63 - i\n",
    "            p1[:, i] = (x1 >> off) & 1\n",
    "            p1_1[:, i] = (x1_1 >> off) & 1\n",
    "        c3=encrypt(p1, subkeys)\n",
    "        c3_1=encrypt(p1_1, subkeys)\n",
    "\n",
    "        c3_xor=np.bitwise_xor(c2,c3)\n",
    "        c3_1_xor=np.bitwise_xor(c2,c3_1)\n",
    "\n",
    "        c3_xor_tran_2,c3_xor_tran=decrypt_1(c3)\n",
    "        c3_1_xor_tran_2,c3_1_xor_tran=decrypt_1(c3_1)\n",
    "\n",
    "        crand=np.concatenate((crand,c3,c3_1),axis=1)\n",
    "#         crand_1=np.concatenate((crand_1,c3_xor_tran_2,c3_1_xor_tran_2),axis=1)\n",
    "#         crand=np.concatenate((crand,np.bitwise_xor(c2_2,c3_xor),np.bitwise_xor(c2_2,c3_1_xor)),axis=1)\n",
    "#         crand_1=np.concatenate((crand_1,np.bitwise_xor(c2_1,c3),np.bitwise_xor(c2_1,c3_1)),axis=1)\n",
    "        # crand_0=np.concatenate((crand_0,c3,c3_1),axis=1)\n",
    "    #生成负样本2\n",
    "\n",
    "\n",
    "#     X = np.concatenate((creal_0,crand_0))\n",
    "    X = np.concatenate((creal,crand))\n",
    "    Yreal  =  np. ones(n)\n",
    "    Yrand  =  np. zeros(n)\n",
    "    Y  =  np. concatenate((Yreal, Yrand))\n",
    "    return X,Y\n",
    "# verify(n=10, nr=31)\n",
    "def make_train_data_mutil(nums,rounds):\n",
    "    master_keys = np.frombuffer(urandom(nums * 80), dtype=np.uint8).reshape(-1, 80) & 1\n",
    "    X,Y=make_train_data(master_keys,nums,rounds)\n",
    "#     X_1,Y_1=make_train_data(master_keys,nums,rounds)\n",
    "#     X_2,Y_2,crd_2=make_train_data(master_keys,nums,rounds)\n",
    "#     X_3,Y_3,crd_3=make_train_data(master_keys,nums,rounds)\n",
    "#     X_test=np.concatenate((X,X_1),axis=1)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBConvBlock(inputs, expansion_ratio, stride, filters, kernel_size):\n",
    "    # 定义扩展通道数\n",
    "    expand_channels = expansion_ratio * inputs.shape[-1]\n",
    "    \n",
    "    # 第一层扩展卷积\n",
    "    x = Conv2D(expand_channels, kernel_size=(1, 1), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    # 深度可分离卷积\n",
    "    x = DepthwiseConv2D(kernel_size, strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x=Dropout(0.5)(x)\n",
    "    # 线性投影回归\n",
    "    x = Conv2D(filters, kernel_size=kernel_size, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "#     x = tf.keras.layers.Add()([x, inputs])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2000\n",
    "# 定义初始学习率、目标学习率和衰减步数\n",
    "initial_lr = 0.002\n",
    "final_lr = 0.0001\n",
    "decay_epochs = 10\n",
    "\n",
    "def decay_learning_rate(epoch, initial_lr, final_lr, decay_epochs):\n",
    "    decay_factor = (final_lr / initial_lr) ** (1 / decay_epochs)\n",
    "    current_lr = initial_lr * (decay_factor ** epoch)\n",
    "    return current_lr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cyclic_lr(epoch, high_lr):\n",
    "    if epoch<=10:\n",
    "        return 0.001 +  (0.0001*(10-epoch))\n",
    "    elif 10<epoch<=25:\n",
    "        return 0.001\n",
    "    else:\n",
    "        res=0.001 -  (0.0001*(epoch-25))\n",
    "        if res>0.0001:\n",
    "            return res\n",
    "        return 0.0001\n",
    "def make_checkpoint(file):\n",
    "    res=ModelCheckpoint(file, monitor='val_loss', save_best_only=True)\n",
    "    return (res)\n",
    "#make residual tower of convolutional blocks\n",
    "def make_resnet(num_words=16,multiset=16, num_filters=16, num_outputs=1, d1=1024, d2=1024, word_size=4, ks=3,depth=5, reg_param=0.00001, final_activation='sigmoid'):\n",
    "  #Input and preprocessing layers\n",
    "  inp = Input(shape=(1*num_words * word_size *multiset ,))\n",
    "  rs = Reshape((4, 16,16))(inp)\n",
    "  perm = Permute((2,3,1))(rs)\n",
    "  input = Conv2D(num_filters , kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(perm)\n",
    "  input = BatchNormalization()(input)\n",
    "  input = Activation('relu')(input)\n",
    "#densenet稠密神经网络\n",
    "  for i in range(depth):\n",
    "        conv1 = BatchNormalization()(input)\n",
    "        conv1 = Activation('relu')(conv1)\n",
    "        conv1 = MBConvBlock(conv1, expansion_ratio=2, stride=1, filters=num_filters, kernel_size=ks)\n",
    "        conv2 = Conv2D(num_filters, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(conv1)\n",
    "        input = tf.keras.layers.Concatenate()([conv2, input]) \n",
    "#         input = tf.keras.layers.Add()([conv2, input]) \n",
    "#         num_filters += 16\n",
    "\n",
    "  conv3 = Conv2D(16, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(input)\n",
    "  conv3 = BatchNormalization()(conv3)\n",
    "#   conv3=Dropout(0.5)(conv3)\n",
    "  conv3 = Activation('relu')(conv3)\n",
    "  \n",
    "  conv4 = Conv2D(8, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(conv3)\n",
    "  conv4 = BatchNormalization()(conv4)\n",
    "#   conv4=Dropout(0.5)(conv4)\n",
    "  conv4 = Activation('relu')(conv4)\n",
    "  \n",
    "  conv5 = Conv2D(4, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(conv4)\n",
    "  conv5 = BatchNormalization()(conv5)\n",
    "#   conv5=Dropout(0.5)(conv5)\n",
    "  conv5 = Activation('relu')(conv5)\n",
    "    \n",
    "  flat1 = Flatten()(conv5)\n",
    "  dense1 = Dense(1024,kernel_regularizer=l2(reg_param))(flat1)\n",
    "  dense1 = BatchNormalization()(dense1)\n",
    "  dense1 = Activation('relu')(dense1)\n",
    "  dense2 = Dense(1024, kernel_regularizer=l2(reg_param))(dense1)\n",
    "  dense2 = BatchNormalization()(dense2)\n",
    "  dense2 = Activation('relu')(dense2)\n",
    "#   dense3=Dense(d1,kernel_regularizer=l2(reg_param))(dense1)\n",
    "#   dense3 = BatchNormalization()(dense3)\n",
    "#   dense3 = Activation('relu')(dense3)\n",
    "#   dense4=Dense(d1,kernel_regularizer=l2(reg_param))(dense3)\n",
    "#   dense4 = BatchNormalization()(dense4)\n",
    "#   dense4 = Activation('relu')(dense4)\n",
    "  dense2=Dropout(0.5)(dense2)\n",
    "  out = Dense(num_outputs, activation=final_activation, kernel_regularizer=l2(reg_param))(dense2)\n",
    "  model = Model(inputs=inp, outputs=out)\n",
    "  return(model)\n",
    "def train_present_distinguisher(num_epochs, num_rounds, depth):\n",
    "    # create the network\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    # TPUStrategy for distributed training\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        print(\"train on tpu\")\n",
    "    else: # default strategy that works on CPU and single GPU\n",
    "      strategy = tf.distribute.get_strategy()\n",
    "    with strategy.scope():\n",
    "        net = make_resnet(depth=depth, reg_param=0.00005)\n",
    "        net.compile(optimizer='adam', loss='mse', metrics=['acc'])\n",
    "    # generate training and validation data and test data\n",
    "    print(\"训练数据生成中。。。\")\n",
    "    X,Y=make_train_data_mutil(2**20,num_rounds)\n",
    "    print(\"训练数据生成完成，生成验证数据中。。。\")\n",
    "    X_eval,Y_eval=make_train_data_mutil(2**16,num_rounds)\n",
    "#     X=np.load(\"/kaggle/input/pressent-def-p/train_data_1.npy\")\n",
    "#     Y = np.load(\"/kaggle/input/pressent-def-p/train_data_lbel_1.npy\")\n",
    "#     X_eval=np.load(\"/kaggle/input/pressent-def-p/test_data.npy\"),\n",
    "#     Y_eval = np.load(\"/kaggle/input/pressent-def-p/test_data_lbel.npy\")\n",
    "    print(\"验证数据生成完成，全部数据生成完成，开始训练\")\n",
    "    # X_test, Y_test = make_train_data(2 ** 8, num_rounds)\n",
    "    # create learnrate schedule\n",
    "#     lr_scheduler = lambda epoch: decay_learning_rate(epoch, initial_lr, final_lr, decay_epochs)\n",
    "    lr = LearningRateScheduler(cyclic_lr)\n",
    "    # train and evaluate\n",
    "\n",
    "    \n",
    "    time_start = time.time()\n",
    "    \n",
    "    h  =  net.fit(X, Y, epochs=num_epochs, batch_size=bs, validation_data=(X_eval, Y_eval), callbacks=[lr])\n",
    "    \n",
    "    time_end = time.time()\n",
    "    total_time = time_end - time_start\n",
    "    # loss, accuracy  =  net. evaluate(X_test, Y_test)\n",
    "    print(\"\\nWhen training for a\", num_rounds, \"round PRESENT \", num_epochs, \"epochs:\")\n",
    "    print(\"\\nBest validation accuracy: \", np.max(h.history['val_acc']))\n",
    "    # print('\\nTest loss:', loss)\n",
    "    # print('\\nTest accuracy:', accuracy)\n",
    "    # f = open(save_path + \"result_for_lyu_train_PRESENT.txt\", \"a\")\n",
    "    print('\\nTotal training time is: %.2f seconds.' % total_time)\n",
    "    return (net, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,history=train_present_distinguisher(50, 5, 10)\n",
    "acc  =  history. history['acc']\n",
    "val_acc  =  history. history['val_acc']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt. figure(figsize=(6, 4))\n",
    "plt. plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt. plot(epochs, val_acc, 'g', label='Validation accuracy')\n",
    "plt. title('Training and validation accuracy')\n",
    "plt. xlabel('Epochs')\n",
    "plt. ylabel('Accuracy')\n",
    "plt. legend(loc='upper left')\n",
    "plt. show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = make_train_data_mutil(2 ** 16, 5)\n",
    "loss, accuracy  =  model. evaluate(X_test, Y_test)\n",
    "print('\\nTest loss:', loss)\n",
    "print('\\nTest accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
